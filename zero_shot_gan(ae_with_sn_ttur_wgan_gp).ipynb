{"cells":[{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load in \n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the \"../input/\" directory.\n# For example, running this (by clicking run or pressing Shift+Enter) will list the files in the input directory\n\nimport os\nprint(os.listdir(\"../input\"))\n\n# Any results you write to the current directory are saved as output.","execution_count":null,"outputs":[]},{"metadata":{"_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","trusted":true},"cell_type":"code","source":"import torch\ntorch.cuda.is_available()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"head = '../input/another-animals-dataset/ex_imgs/'\n\ncat_list = list(sorted(os.listdir(head)))\ncat_list","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"class_csv = pd.read_csv('../input/another-animals-dataset/labels/classes.txt', sep='\\t', header=None)\nclass_csv.sample(5)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"class_to_label_dict = {}\nfor cls in class_csv.values:\n    class_to_label_dict[cls[1]] = cls[0] - 1\n\nlabel_to_class_dict = {v: k for k, v in class_to_label_dict.items()}","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"label_to_class_dict[0], class_to_label_dict[label_to_class_dict[0]]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"cat_ = ['bobcat']","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"img_list = []\nlabel_list = []\n\n\nfor cat in cat_:\n    c_img_list = [head + cat + '/' + ipl for ipl in os.listdir(head + cat)]\n    img_list += c_img_list\n    label_list += [class_to_label_dict[cat]] * len(c_img_list)\n    \nlen(img_list), img_list[0], len(label_list), label_list[0]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"a_head = '../input/awa-dataset/imgs_e/'","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"for cat in cat_:\n    c_img_list = [a_head + cat + '/' + ipl for ipl in os.listdir(a_head + cat)]\n    img_list += c_img_list\n    label_list += [class_to_label_dict[cat]] * len(c_img_list)\n    \nlen(img_list), img_list[-1], len(label_list), label_list[-1]","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### try imgs..."},{"metadata":{"trusted":true},"cell_type":"code","source":"from PIL import Image, ImageOps, ImageEnhance\n\nimport matplotlib.pyplot as plt\n\nimg = Image.open(img_list[0])\nplt.imshow(img)\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"img = Image.open(img_list[-1])\nplt.imshow(img)\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"import torch\nfrom torch import optim\nfrom torch import nn\nimport torch.nn.functional as F\nfrom torch.utils.data import DataLoader, Dataset\nfrom torchvision import models\nimport torchvision.transforms as transforms","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"!pip install torchsummary","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from torchsummary import summary","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"class AwaDataSet(Dataset):\n    #  img_path_list: text. labels: text, need to locate through\n    def __init__(self, img_path_list, x_transform):\n        self.img_path_list = img_path_list\n        self.x_transform = x_transform\n\n    def __getitem__(self, idx):\n        \n        try:\n            img = Image.open(self.img_path_list[idx])\n        except:\n            img = Image.fromarray(np.random.randint(0, 255, (64, 64, 3), dtype=np.uint8))\n                \n        if head in self.img_path_list[idx]:\n            if np.random.randint(1000) <= 125:\n                img = ImageOps.mirror(img)\n\n            if np.random.randint(1000) <= 50:\n                img = img.rotate(np.random.randint(-18, 18))\n\n        else:\n            img = img.convert('RGBA')\n            \n            blank = img.copy()\n        \n            if np.random.randint(1000) <= 125:\n                img = ImageOps.mirror(img)\n\n            if np.random.randint(1000) <= 50:\n                img = img.rotate(np.random.randint(-18, 18), expand=True)\n                \n            width, height = img.size\n            target_size = max(img.size)\n\n            blank = blank.resize((target_size * 3, target_size * 3))\n            blank = blank.crop((target_size, target_size, 2 * target_size, 2 * target_size))\n            \n            blank.paste(img, (int((max(img.size) - width) / np.random.uniform(1.75, 2.25)),\n                          int((max(img.size) - height) / np.random.uniform(1.75, 2.25))), img)\n            img = blank\n            \n        img = img.convert('RGB')\n        img = img.resize((128, 128))\n        img = np.array(img, dtype=np.float32) / 255\n\n        if np.random.randint(1000) <= 125:\n            img = np.clip(img * np.random.uniform(0.8, 1.28), 0, 1)\n        \n        img = self.x_transform(img)\n        \n#         label = self.label_list[idx]\n\n        return img\n\n    def __len__(self):\n        return len(self.img_path_list)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"transform = transforms.Compose([\n    transforms.ToTensor(),\n    transforms.Normalize(mean=(0.5,) * 3, std=(0.5,) * 3),\n])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train_dataset = AwaDataSet(img_list, transform)\ntrain_loader = DataLoader(train_dataset, batch_size=32, shuffle=True)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"for db in train_loader:\n    img = db\n#     print(img.mean())\n#     print(label)\n#     print(label)\n\n    img = img[0].cpu().data.numpy()\n    img = np.transpose(img, (1, 2, 0))\n    plt.imshow(img * 0.5 + 0.5)\n    plt.show()\n\n    break","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### weight initialized function"},{"metadata":{"trusted":true},"cell_type":"code","source":"# custom weights initialization called on netG and netD\ndef weights_init(m):\n    classname = m.__class__.__name__\n    if classname.find('Conv') != -1:\n        nn.init.normal_(m.weight.data, 0.0, 0.02)\n    elif classname.find('BatchNorm') != -1:\n        nn.init.normal_(m.weight.data, 1.0, 0.02)\n        nn.init.constant_(m.bias.data, 0)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### crazy idea... again..."},{"metadata":{},"cell_type":"markdown","source":"### Define the auto_encoder(this one differs to the vae one, this one contains only the encoder part, and the decoder also works as the generator...)"},{"metadata":{"trusted":true},"cell_type":"code","source":"class auto_encoder(nn.Module):\n    def __init__(self):\n        super(auto_encoder, self).__init__()\n        \n        self.encoder = nn.Sequential(\n            nn.Conv2d(3, 32, 4, 2, 1),\n            nn.LeakyReLU(0.2, inplace=True),\n            # size: 64 x 64\n            \n            nn.Conv2d(32, 64, 4, 2, 1),\n            nn.InstanceNorm2d(64),\n            nn.LeakyReLU(0.2, inplace=True),\n            # size: 32 x 32\n            \n            nn.Conv2d(64, 128, 4, 2, 1),\n            nn.InstanceNorm2d(128),\n            nn.LeakyReLU(0.2, inplace=True),\n            # size: 16 x 16 \n            \n            nn.Conv2d(128, 256, 4, 2, 1),\n            nn.InstanceNorm2d(256),\n            nn.LeakyReLU(0.2, inplace=True),\n            nn.Dropout(0.5),\n            # size: 8 x 8\n            \n            nn.Conv2d(256, 512, 4, 2, 1),\n            nn.InstanceNorm2d(512),\n            nn.LeakyReLU(0.2, inplace=True),\n            nn.Dropout(0.5),\n            # size: 4 x 4\n            \n            nn.Conv2d(512, 1024, 4, 2, 1),\n            nn.InstanceNorm2d(1024),\n            nn.LeakyReLU(0.2, inplace=True),\n            nn.Dropout(0.5),\n            # size: 2 x 2\n            \n            nn.Conv2d(1024, 2048, 4, 2, 1),\n            # size: 1 x 1\n        )\n        \n        self.mu = nn.Linear(2048, 256)\n        self.logvar = nn.Linear(2048, 256)\n            \n    def encode(self, x):\n        x = self.encoder(x)\n        x = x.view(-1, 2048)\n        \n        return self.mu(x), self.logvar(x)\n    \n    def reparameterize(self, mu, logvar):\n        std = torch.exp(0.5 * logvar)\n        eps = torch.randn_like(std)\n        \n        return mu + eps * std\n        \n    def forward(self, x):\n        x = x.view(-1, 3, 128, 128)\n        mu, logvar = self.encode(x)\n        x = self.reparameterize(mu, logvar)\n#         x = self.encoder(x)\n        \n        x = (x - x.mean(dim=1, keepdim=True)) / (x.std(dim=1, keepdim=True) + 1e-8)\n        x = x.view(-1, 256, 1, 1)        \n                \n        return x, mu, logvar","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"AE = auto_encoder().cuda()\nsummary(AE, (3, 128, 128))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"AE.apply(weights_init)\nprint('init...')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Loss Function"},{"metadata":{"trusted":true},"cell_type":"code","source":"def criterion_vae(rec_imgs, imgs, mu, logvar):\n    \n    ### remember: the range of output image is (-1, 1),\n    ### without making it to range(0, 1), using bce_loss would raise some strange error.\n    \n#     print(rec_imgs.max(), rec_imgs.min(), imgs.max(), imgs.min())\n    bce_loss = F.binary_cross_entropy(rec_imgs.view(-1, 3 * 128 * 128), imgs.view(-1, 3 * 128 * 128), reduction='sum')\n    \n    kld_loss = -0.5 * torch.sum(1 + logvar - mu ** 2 - logvar.exp())\n    \n    return (bce_loss + kld_loss) / rec_imgs.size()[0]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from torch.nn import Parameter\n\ndef l2normalize(v, eps=1e-12):\n    return v / (v.norm() + eps)\n\n\nclass SpectralNorm(nn.Module):\n    def __init__(self, module, name='weight', power_iterations=1):\n        super(SpectralNorm, self).__init__()\n        self.module = module\n        self.name = name\n        self.power_iterations = power_iterations\n        if not self._made_params():\n            self._make_params()\n\n    def _update_u_v(self):\n        u = getattr(self.module, self.name + \"_u\")\n        v = getattr(self.module, self.name + \"_v\")\n        w = getattr(self.module, self.name + \"_bar\")\n\n        height = w.data.shape[0]\n        for _ in range(self.power_iterations):\n            v.data = l2normalize(torch.mv(torch.t(w.view(height,-1).data), u.data))\n            u.data = l2normalize(torch.mv(w.view(height,-1).data, v.data))\n\n        # sigma = torch.dot(u.data, torch.mv(w.view(height,-1).data, v.data))\n        sigma = u.dot(w.view(height, -1).mv(v))\n        setattr(self.module, self.name, w / sigma.expand_as(w))\n\n    def _made_params(self):\n        try:\n            u = getattr(self.module, self.name + \"_u\")\n            v = getattr(self.module, self.name + \"_v\")\n            w = getattr(self.module, self.name + \"_bar\")\n            return True\n        except AttributeError:\n            return False\n\n\n    def _make_params(self):\n        w = getattr(self.module, self.name)\n\n        height = w.data.shape[0]\n        width = w.view(height, -1).data.shape[1]\n\n        u = Parameter(w.data.new(height).normal_(0, 1), requires_grad=False)\n        v = Parameter(w.data.new(width).normal_(0, 1), requires_grad=False)\n        u.data = l2normalize(u.data)\n        v.data = l2normalize(v.data)\n        w_bar = Parameter(w.data)\n\n        del self.module._parameters[self.name]\n\n        self.module.register_parameter(self.name + \"_u\", u)\n        self.module.register_parameter(self.name + \"_v\", v)\n        self.module.register_parameter(self.name + \"_bar\", w_bar)\n\n\n    def forward(self, *args):\n        self._update_u_v()\n        return self.module.forward(*args)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Define self attention layer"},{"metadata":{"trusted":true},"cell_type":"code","source":"class Self_Attn(nn.Module):\n    def __init__(self, in_dim):\n        super(Self_Attn, self).__init__()\n        self.channel_in = in_dim\n        \n        self.Q_conv = nn.Conv2d(in_dim, in_dim//8, 1, 1, 0)\n        self.K_conv = nn.Conv2d(in_dim, in_dim//8, 1, 1, 0)\n        self.V_conv = nn.Conv2d(in_dim, in_dim, 1, 1, 0)\n        \n        self.gamma = nn.Parameter(torch.zeros(1))\n        \n        self.softmax = nn.Softmax(dim=-1)\n        \n    def forward(self, x):\n        b_size, c, width, height = x.size()\n        \n        proj_Q = self.Q_conv(x).view(b_size, -1, width * height).permute(0, 2, 1)\n        proj_K = self.K_conv(x).view(b_size, -1, width * height)\n        \n        energy = torch.bmm(proj_Q, proj_K)\n        attn = self.softmax(energy)\n        \n        proj_V = self.V_conv(x).view(b_size, -1, width * height)\n        \n        out = torch.bmm(proj_V, attn.permute(0, 2, 1))\n        out = out.view(b_size, c, width, height)\n        \n        out = self.gamma * out + x\n        \n        return out","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Define Generator"},{"metadata":{"trusted":true},"cell_type":"code","source":"class generator(nn.Module):\n    def __init__(self):\n        super(generator, self).__init__()\n        \n        self.main = nn.Sequential(\n            SpectralNorm(nn.ConvTranspose2d(512, 1024, 4, 1, 0, bias=False)),\n            nn.BatchNorm2d(1024),\n            nn.LeakyReLU(0.2, inplace=True),\n            # size: 4 x 4\n\n            SpectralNorm(nn.ConvTranspose2d(1024, 512, 4, 2, 1, bias=False)),\n            nn.BatchNorm2d(512),\n            nn.LeakyReLU(0.2, inplace=True),\n            # size: 8 x 8\n\n            SpectralNorm(nn.ConvTranspose2d(512, 256, 4, 2, 1, bias=False)),\n            nn.BatchNorm2d(256),\n            nn.LeakyReLU(0.2, inplace=True),\n            nn.Dropout(0.25),\n            # size: 16 x 16\n\n            SpectralNorm(nn.ConvTranspose2d(256, 128, 4, 2, 1, bias=False)),\n            nn.BatchNorm2d(128),\n            nn.LeakyReLU(0.2, inplace=True),\n#             Self_Attn(128),\n            nn.Dropout(0.25),\n            # size: 32 x 32\n            \n            SpectralNorm(nn.ConvTranspose2d(128, 64, 4, 2, 1, bias=False)),\n            nn.BatchNorm2d(64),\n            nn.LeakyReLU(0.2, inplace=True),\n#             Self_Attn(64),\n            nn.Dropout(0.25),\n            # size: 64 x 64     \n            \n#             nn.ConvTranspose2d(64, 3, 4, 2, 1, bias=False),\n#             nn.Tanh()\n            # size: 128 x 128 \n        )\n        \n        self.gen_output = nn.ConvTranspose2d(64, 3, 4, 2, 1, bias=False)\n        self.ae_output = nn.ConvTranspose2d(64, 3, 4, 2, 1, bias=False)\n        self.tanh = nn.Tanh()\n        self.sigmoid = nn.Sigmoid()\n        \n        \n    def forward(self, x, mode='gen'):  \n        \n        x = self.main(x)\n        \n        if mode == 'gen':\n            x = self.gen_output(x)\n            x = self.tanh(x)\n            \n        elif mode == 'ae':\n            x = self.ae_output(x)\n            x = self.sigmoid(x)\n        \n        return x","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"G = generator().cuda()\nsummary(G, [(512, 1, 1)])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"G.apply(weights_init)\nprint('init...')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### define discriminator"},{"metadata":{"trusted":true},"cell_type":"code","source":"class discriminator(nn.Module):\n    def __init__(self):\n        super(discriminator, self).__init__()\n        self.main = nn.Sequential(\n            SpectralNorm(nn.Conv2d(3, 32, 4, 2, 1, bias=False)),\n            nn.LeakyReLU(0.2, inplace=True),\n            # size: 64 x 64\n            \n            SpectralNorm(nn.Conv2d(32, 64, 4, 2, 1, bias=False)),\n#             nn.BatchNorm2d(64),\n            nn.InstanceNorm2d(64),\n            nn.LeakyReLU(0.2, inplace=True),\n            # size: 32 x 32\n            \n            SpectralNorm(nn.Conv2d(64, 128, 4, 2, 1, bias=False)),\n#             nn.BatchNorm2d(128),\n            nn.InstanceNorm2d(128),\n            nn.LeakyReLU(0.2, inplace=True),\n            # size: 16 x 16\n            \n            SpectralNorm(nn.Conv2d(128, 256, 4, 2, 1, bias=False)),\n#             nn.BatchNorm2d(256),\n            nn.InstanceNorm2d(256),\n            nn.LeakyReLU(0.2, inplace=True),\n            nn.Dropout(0.25),\n            # size: 8 x 8\n            \n            SpectralNorm(nn.Conv2d(256, 512, 4, 2, 1, bias=False)),\n#             nn.BatchNorm2d(512),\n            nn.InstanceNorm2d(512),\n            nn.LeakyReLU(0.2, inplace=True),\n            nn.Dropout(0.25),\n#             Self_Attn(512),\n            # size: 4 x 4\n            \n            SpectralNorm(nn.Conv2d(512, 1024, 4, 2, 1, bias=False)),\n#             nn.BatchNorm2d(1024),\n            nn.InstanceNorm2d(1024),\n            nn.LeakyReLU(0.2, inplace=True),\n            nn.Dropout(0.25),\n#             Self_Attn(1024),\n            # size: 2 x 2\n            \n#             nn.Conv2d(1024, 50 + 1, 4, 2, 1, bias=False),\n            nn.Conv2d(1024, 1, 4, 2, 1, bias=False),\n\n#             nn.Sigmoid(),\n        )\n                \n    def forward(self, x):\n        x = self.main(x)\n#         x = x.view(-1, 50 + 1)\n        x = x.view(-1)\n        \n        return x","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"D = discriminator().cuda()\nsummary(D, (3, 128, 128))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"D.apply(weights_init)\nprint('init...')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### gp loss"},{"metadata":{"trusted":true},"cell_type":"code","source":"from torch import autograd\ndef cal_gp(d_model, real_imgs, fake_imgs):\n    \n    b_size = real_imgs.size()[0]\n    \n    alpha = torch.rand(b_size, 1, 1, 1)\n    alpha = alpha.expand_as(real_imgs).cuda()\n    \n    inter = alpha * real_imgs + (1 - alpha) * fake_imgs\n    inter = inter.cuda()\n    inter = autograd.Variable(inter, requires_grad=True)\n    \n    disc = d_model(inter)\n    \n    grads = autograd.grad(outputs=disc, inputs=inter,\n                         grad_outputs=torch.ones_like(disc).cuda(),\n                         create_graph=True, retain_graph=True, only_inputs=True)[0]\n    grads = grads.view(grads.size(0), -1)\n    \n    gp = ((grads.norm(2, dim=1) - 1) ** 2).mean()\n    return gp","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"criterion_bce = nn.BCELoss()\ncriterion_mse = nn.MSELoss()\ncriterion_ce = nn.CrossEntropyLoss()\n\ng_optimizer = optim.Adam(G.parameters(), lr=5e-5, betas=(0.5, 0.9))\nd_optimizer = optim.Adam(D.parameters(), lr=2e-4, betas=(0.5, 0.9))\n# g_optimizer = optim.RMSprop(G.parameters(), lr=1e-5)\n# d_optimizer = optim.RMSprop(D.parameters(), lr=1e-5)\n\nae_optimizer = optim.Adam(AE.parameters(), lr=1e-4, betas=(0.5, 0.9))\nag_optimizer = optim.Adam(G.parameters(), lr=1e-4, betas=(0.5, 0.9))\n\n# ae_scheduler = lr_scheduler.ExponentialLR(ae_optimizer, gamma=0.999)\n# ag_scheduler = lr_scheduler.ExponentialLR(ag_optimizer, gamma=0.999)\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"import matplotlib.pyplot as plt\nfrom pylab import rcParams\nrcParams['figure.figsize'] = 6.4, 6.4","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"d_epoch = 1\ng_epoch = 1\nae_epoch = 1","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"nc = 2e-1\ndecay = 0.9995\n\nfor epoch in range(1000):\n    \n    d_running_loss = 0.0\n    g_running_loss = 0.0\n    ae_running_loss = 0.0\n    \n    if epoch % 10 == 9 or epoch == 0:\n        print('nc:', nc)\n    \n    for i, data_batch in enumerate(train_loader):\n        real_imgs = data_batch\n        real_imgs = real_imgs.cuda()\n        \n        b_size = real_imgs.size()[0]\n        \n        labels_real = torch.ones(b_size).cuda()\n        labels_fake = torch.zeros(b_size).cuda()\n        labels_real_ = torch.ones(b_size).cuda()\n        \n        labels_real_s = 0.7 * torch.ones(b_size).cuda() + 0.55 * torch.rand(b_size).cuda()\n        labels_fake_s = 0.25 * torch.rand(b_size).cuda()\n        \n        if np.random.randint(1000) < 100:\n            labels_real, labels_fake = labels_fake, labels_real\n            labels_real_s, labels_fake_s = labels_fake_s, labels_real_s\n       \n        for _ in range(ae_epoch):\n            AE.zero_grad()\n            G.zero_grad()\n\n#             imgs_enc, mu, logvar = AE(real_imgs)\n            imgs_enc, mu, logvar = AE(real_imgs)\n            z = torch.randn(b_size, 256, 1, 1).cuda()\n            imgs_enc_ = torch.cat((z, imgs_enc), dim=1)\n        \n            rec_imgs = G(imgs_enc_, mode='ae')\n    #         loss = criterion(rec_imgs.view(b_size, -1) * 0.5 + 0.5, real_imgs.view(b_size, -1) * 0.5 + 0.5)\n            ae_loss = 1e-2 * criterion_vae(rec_imgs, real_imgs * 0.5 + 0.5, mu, logvar)\n#             ae_loss = criterion_bce(rec_imgs, real_imgs * 0.5 + 0.5)\n\n            ae_loss.backward()\n\n            ae_optimizer.step()\n            ag_optimizer.step()\n\n            ae_running_loss += ae_loss.item()\n                        \n        for _ in range(d_epoch):\n            D.zero_grad()\n            \n            d_real_outputs = D(real_imgs + nc * torch.randn_like(real_imgs).cuda())\n#             d_real_loss = criterion_mse(d_real_outputs, labels_real_s)\n            d_real_loss = d_real_outputs.view(-1).mean()\n            \n            z = torch.randn(b_size, 512, 1, 1).cuda()\n#             fake_enc = RE(z).detach()\n            fake_imgs = G(z, mode='gen').detach()\n            \n            d_fake_outputs = D(fake_imgs + nc * torch.randn_like(fake_imgs).cuda())\n#             d_fake_loss = criterion_mse(d_fake_outputs, labels_fake_s)\n            d_fake_loss = d_fake_outputs.view(-1).mean()\n    \n            d_gp_loss = cal_gp(D, real_imgs, fake_imgs)\n            \n            d_loss = d_fake_loss - d_real_loss + 10 * d_gp_loss\n            \n#             d_loss = d_real_loss + d_fake_loss\n            d_loss.backward(retain_graph=True)\n            \n            d_optimizer.step()\n            \n            d_running_loss += d_loss.item()\n            \n            nc *= decay\n            \n        for _ in range(g_epoch):\n            G.zero_grad()\n            \n            z = torch.randn(b_size, 512, 1, 1).cuda()\n            fake_imgs = G(z, mode='gen')\n            \n            d_outputs = D(fake_imgs)\n            \n#             g_loss = criterion_mse(d_outputs, labels_real_)\n            g_loss = - d_outputs.view(-1).mean()\n            g_loss.backward()\n            \n            g_optimizer.step()\n            \n            g_running_loss += g_loss.item()\n        \n        t = 30\n        if (epoch % 10 == 9 or epoch == 0) and i == 0:\n#         if i % t == t-1 or i == 0:\n\n            print(epoch+1, (i+1) * 32, \n                    'd_running_loss:', d_running_loss/ (t if i != 0 else 1),\n                    'g_running_loss:', g_running_loss/ (t if i != 0 else 1),\n                 )\n            print('ae_running_loss:', ae_running_loss/ (t if i != 0 else 1))\n                        \n            d_running_loss = 0.0\n            g_running_loss = 0.0\n            ae_running_loss = 0.0\n            \n            real_samples = real_imgs[:4]\n            real_samples_ = real_samples.cpu().data.numpy()\n            real_samples_ = np.transpose(real_samples_, (0, 2, 3, 1))            \n            \n            plt.subplots_adjust(wspace=0.025, hspace=0.025)\n            plt.grid(False)\n            for k in range(4):\n                plt.subplot(1, 4, k+1)\n#                 plt.title(label_to_class_dict[f_labels[k]])\n                plt.axis('off')\n\n                plt.imshow(real_samples_[k] * 0.5 + 0.5)\n            plt.show()\n            \n            enc_samples, _, _ = AE(real_samples)\n            z = torch.randn(4, 256, 1, 1).cuda()\n            enc_samples_ = torch.cat((z, enc_samples), dim=1)\n            rec_samples = G(enc_samples_, mode='ae')\n            rec_samples_ = rec_samples.cpu().data.numpy()\n            rec_samples_ = np.transpose(rec_samples_, (0, 2, 3, 1))\n            \n            plt.subplots_adjust(wspace=0.025, hspace=0.025)\n            plt.grid(False)\n            for k in range(4):\n                plt.subplot(1, 4, k+1)\n#                 plt.title(label_to_class_dict[f_labels[k]])\n                plt.axis('off')\n\n                plt.imshow(rec_samples_[k])\n            plt.show()\n            \n            z = torch.randn(4, 512, 1, 1).cuda()\n            fake_samples = G(z).cpu().data.numpy()\n            fake_samples = np.transpose(fake_samples, (0, 2, 3, 1))\n            \n            plt.subplots_adjust(wspace=0.025, hspace=0.025)\n            plt.grid(False)\n            for k in range(4):\n                plt.subplot(1, 4, k+1)\n#                 plt.title(label_to_class_dict[f_labels[k]])\n                plt.axis('off')\n\n                plt.imshow(fake_samples[k] * 0.5 + 0.5)\n            plt.show()\n            \n            \n\n#         break\n#     break","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"name":"python","version":"3.6.4","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat":4,"nbformat_minor":1}